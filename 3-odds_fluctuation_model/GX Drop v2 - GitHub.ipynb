{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GX Quantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skellam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame we will be working with in this project has been the subject of study for an extended period. It contains over 200 features that can be utilized depending on the context. For this specific project, we will be using only a few of these features that are relevant to the problem we aim to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataframe\n",
    "df = pd.read_csv(\"www-datafoot-org.csv\")\n",
    "\n",
    "# Converting 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Dropping rows with missing values in 'FTHG' and 'FTAG'. \n",
    "# Rows with missing values in these columns indicate future matches that will not be used here\n",
    "df.dropna(subset=['FTHG', 'FTAG'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the league division categories that we will work on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame contains data for over 100 leagues, all available at [datafoot.org](www.datafoot.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each league has been categorized based on its global relevance. The categories range from 1 to 5, with 1 being the most recognized and 5 being the least known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting specific division ranks\n",
    "df = df.loc[df['Div Rank'].isin(['1 - EliteMáx', '2 - Destaque', \"3 - Eminente\", \"4 - Várzea\"])].copy()\n",
    "\n",
    "# Resetting index\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "____________________\n",
    "____________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop v2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_bydate(df, date_column, cutoff_date):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into two based on a cutoff date, prints the number of rows in each part, and returns the DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The original DataFrame.\n",
    "    - date_column: The name of the column in the DataFrame that contains the dates.\n",
    "    - cutoff_date: The cutoff date for the division in string format, 'YYYY-MM-DD'.\n",
    "    \n",
    "    Returns:\n",
    "    - df_train: DataFrame containing the rows with dates before the cutoff date.\n",
    "    - df_final_test: DataFrame containing the rows with dates on or after the cutoff date.\n",
    "    \"\"\"\n",
    "    # Converting the date column to datetime if not already\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Defining the cutoff date\n",
    "    cutoff_date = pd.Timestamp(cutoff_date)\n",
    "    \n",
    "    # Splitting the DataFrame\n",
    "    df_final_test = df[df[date_column] >= cutoff_date].copy()\n",
    "    df_train = df[df[date_column] < cutoff_date].copy()\n",
    "    \n",
    "    # Printing the number of rows in each resulting DataFrame\n",
    "    print(\"Rows in the training DataFrame:\", df_train.shape[0])\n",
    "    print(\"Rows in the final test DataFrame:\", df_final_test.shape[0])\n",
    "    \n",
    "    # Returning the split DataFrames\n",
    "    return df_train, df_final_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Processing Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Analyzing the dataframe and selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Unnamed: 0', 'Unnamed: 0.1', 'id', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'ht_goals_h', 'ht_goals_a', 'oddH_op', 'oddD_op', 'oddA_op', 'oddH', 'oddD', 'oddA', 'Ah_op', 'oddAHH_op', 'oddAHA_op', 'Ah', 'oddAHH', 'oddAHA', 'AhOU_op', 'oddAHOver_op', 'oddAHUnder_op', 'AhOU', 'oddAHOver', 'oddAHUnder', 'Div Rank', 'FTR', 'count_div', 'pHome', 'pDraw', 'pAway', 'pHome_clo', 'pDraw_clo', 'pAway_clo', 'pHome_clonj', 'pDraw_clonj', 'pAway_clonj', 'juice', 'pts_h', 'pts_a', 'oddOver_nojuice', 'oddUnder_nojuice', 'OverScore', 'oddOver_clo_nojuice', 'oddUnder_clo_nojuice', 'OverScore_clo', 'PLH', 'PLD', 'PLA', 'PLH_op', 'PLD_op', 'PLA_op', 'odd12_pin_op', 'odd12_pin_clo', 'PL12_op', 'PL12_clo', 'dif', 'PL_Ahh_op', 'PL_Aha_op', 'PL_Ahh', 'PL_Aha', 'total_goals', 'PL_AhOver_op', 'PL_AhUnder_op', 'PL_AhOver', 'PL_AhUnder', 'contador_home', 'contador_away', 'dif5', 'dif5_away', 'difedge_h', 'difedge_a', 'difedgew_clo_h', 'difedgew_clo_a', 'factor_home', 'factor_away', 'factored_goals_home', 'factored_goals_away', 'valor_gol_home', 'valor_gol_away', 'custo_gol_home', 'custo_gol_away', 'unidif_home', 'unidif_away', 'unidade_gol_home', 'unidade_gol_away', 'unidade_gol_clo_home', 'unidade_gol_clo_away', 'dif_unidade_gol_home', 'dif_unidade_gol_away', 'typoH', 'typoA', 'm5_difedge_h', 'cv5_difedge_h', 'm10_difedge_h', 'cv10_difedge_h', 'm20_difedge_h', 'cv20_difedge_h', 'm5_difedge_a', 'cv5_difedge_a', 'm10_difedge_a', 'cv10_difedge_a', 'm20_difedge_a', 'cv20_difedge_a', 'm5_difedgew_clo_h', 'cv5_difedgew_clo_h', 'm10_difedgew_clo_h', 'cv10_difedgew_clo_h', 'm20_difedgew_clo_h', 'cv20_difedgew_clo_h', 'm5_difedgew_clo_a', 'cv5_difedgew_clo_a', 'm10_difedgew_clo_a', 'cv10_difedgew_clo_a', 'm20_difedgew_clo_a', 'cv20_difedgew_clo_a', 'ex5_difedgew_clop_h', 'ex5_difedgew_clop_a', 'ex10_difedgew_clop_h', 'ex10_difedgew_clop_a', 'ex20_difedgew_clop_h', 'ex20_difedgew_clop_a', 'm5_pts_home', 'm5_pts_away', 'cv5_pts_home', 'cv5_pts_away', 'm10_pts_home', 'm10_pts_away', 'cv10_pts_home', 'cv10_pts_away', 'evo_pts_home', 'evo_pts_away', 'm5_goals_scored_home', 'cv5_goals_scored_home', 'm5_goals_scored_away', 'cv5_goals_scored_away', 'm10_goals_scored_home', 'cv10_goals_scored_home', 'm10_goals_scored_away', 'cv10_goals_scored_away', 'cvclass5_goals_scored_home', 'cvclass5_goals_scored_away', 'cvclass10_goals_scored_home', 'cvclass10_goals_scored_away', 'm5_goals_conceded_home', 'cv5_goals_conceded_home', 'm5_goals_conceded_away', 'cv5_goals_conceded_away', 'm10_goals_conceded_home', 'cv10_goals_conceded_home', 'm10_goals_conceded_away', 'cv10_goals_conceded_away', 'cvclass5_goals_conceded_home', 'cvclass5_goals_conceded_away', 'cvclass10_goals_conceded_home', 'cvclass10_goals_conceded_away', 'm5_factor_scored_home', 'cv5_factor_scored_home', 'm5_factor_scored_away', 'cv5_factor_scored_away', 'm10_factor_scored_home', 'cv10_factor_scored_home', 'm10_factor_scored_away', 'cv10_factor_scored_away', 'cvclass5_factor_scored_home', 'cvclass5_factor_scored_away', 'cvclass10_factor_scored_home', 'cvclass10_factor_scored_away', 'm5_factor_conceded_home', 'cv5_factor_conceded_home', 'm5_factor_conceded_away', 'cv5_factor_conceded_away', 'm10_factor_conceded_home', 'cv10_factor_conceded_home', 'm10_factor_conceded_away', 'cv10_factor_conceded_away', 'cvclass5_factor_conceded_home', 'cvclass5_factor_conceded_away', 'cvclass10_factor_conceded_home', 'cvclass10_factor_conceded_away', 'exfg5_scored_home', 'exfg5_scored_away', 'exfg5_conceded_home', 'exfg5_conceded_away', 'exfg5_M_scored_home', 'exfg5_M_scored_away', 'exfg5_dif', 'exfg5_M_dif', 'exfg10_scored_home', 'exfg10_scored_away', 'exfg10_conceded_home', 'exfg10_conceded_away', 'exfg10_M_scored_home', 'exfg10_M_scored_away', 'exfg10_dif', 'exfg10_M_dif', 'exfg5_M_dif_ligtypo_ratio', 'exfg10_M_dif_ligtypo_ratio', 'm5_uni_scored_home', 'cv5_uni_scored_home', 'm5_uni_scored_away', 'cv5_uni_scored_away', 'm10_uni_scored_home', 'cv10_uni_scored_home', 'm10_uni_scored_away', 'cv10_uni_scored_away', 'cvclass5_uni_scored_home', 'cvclass5_uni_scored_away', 'cvclass10_uni_scored_home', 'cvclass10_uni_scored_away', 'm5_uni_conceded_home', 'cv5_uni_conceded_home', 'm5_uni_conceded_away', 'cv5_uni_conceded_away', 'm10_uni_conceded_home', 'cv10_uni_conceded_home', 'm10_uni_conceded_away', 'cv10_uni_conceded_away', 'cvclass5_uni_conceded_home', 'cvclass5_uni_conceded_away', 'cvclass10_uni_conceded_home', 'cvclass10_uni_conceded_away', 'm5_uniclo_scored_home', 'cv5_uniclo_scored_home', 'm5_uniclo_scored_away', 'cv5_uniclo_scored_away', 'm10_uniclo_scored_home', 'cv10_uniclo_scored_home', 'm10_uniclo_scored_away', 'cv10_uniclo_scored_away', 'cvclass5_uniclo_scored_home', 'cvclass5_uniclo_scored_away', 'cvclass10_uniclo_scored_home', 'cvclass10_uniclo_scored_away', 'm5_uniclo_conceded_home', 'cv5_uniclo_conceded_home', 'm5_uniclo_conceded_away', 'cv5_uniclo_conceded_away', 'm10_uniclo_conceded_home', 'cv10_uniclo_conceded_home', 'm10_uniclo_conceded_away', 'cv10_uniclo_conceded_away', 'cvclass5_uniclo_conceded_home', 'cvclass5_uniclo_conceded_away', 'cvclass10_uniclo_conceded_home', 'cvclass10_uniclo_conceded_away', 'm5_valor_scored_home', 'cv5_valor_scored_home', 'm5_valor_scored_away', 'cv5_valor_scored_away', 'm10_valor_scored_home', 'cv10_valor_scored_home', 'm10_valor_scored_away', 'cv10_valor_scored_away', 'cvclass5_valor_scored_home', 'cvclass5_valor_scored_away', 'cvclass10_valor_scored_home', 'cvclass10_valor_scored_away', 'm5_valor_conceded_home', 'cv5_valor_conceded_home', 'm5_valor_conceded_away', 'cv5_valor_conceded_away', 'm10_valor_conceded_home', 'cv10_valor_conceded_home', 'm10_valor_conceded_away', 'cv10_valor_conceded_away', 'cvclass5_valor_conceded_home', 'cvclass5_valor_conceded_away', 'cvclass10_valor_conceded_home', 'cvclass10_valor_conceded_away', 'exvg10_scored_home', 'exvg10_scored_away', 'exvg10_dif', 'exvg5_scored_home', 'exvg5_scored_away', 'exvg5_dif', 'exuni10_scored_home', 'exuni10_scored_away', 'exuni10_dif', 'exuni5_scored_home', 'exuni5_scored_away', 'exuni5_dif', 'exuniclo10_scored_home', 'exuniclo10_scored_away', 'exuniclo10_dif', 'ex_uniclop10_scored_home', 'ex_uniclop10_scored_away', 'ex_uniclop10_dif', 'm5_custo_scored_home', 'cv5_custo_scored_home', 'm5_custo_scored_away', 'cv5_custo_scored_away', 'm10_custo_scored_home', 'cv10_custo_scored_home', 'm10_custo_scored_away', 'cv10_custo_scored_away', 'cvclass5_custo_scored_home', 'cvclass5_custo_scored_away', 'cvclass10_custo_scored_home', 'cvclass10_custo_scored_away', 'm5_custo_conceded_home', 'cv5_custo_conceded_home', 'm5_custo_conceded_away', 'cv5_custo_conceded_away', 'm10_custo_conceded_home', 'cv10_custo_conceded_home', 'm10_custo_conceded_away', 'cv10_custo_conceded_away', 'cvclass5_custo_conceded_home', 'cvclass5_custo_conceded_away', 'cvclass10_custo_conceded_home', 'cvclass10_custo_conceded_away', 'excg10_scored_home', 'excg10_scored_away', 'excg10_dif', 'excg5_scored_home', 'excg5_scored_away', 'excg5_dif', 'ex2cg10_scored_home', 'ex2cg10_scored_away', 'ex2cg10_dif', 'ex2cg5_scored_home', 'ex2cg5_scored_away', 'ex2cg5_dif', 'm5_dif_unidade_gol_home', 'm5_dif_unidade_gol_away', 'm10_dif_unidade_gol_home', 'm10_dif_unidade_gol_away', 'exunidif10_home', 'exunidif10_away', 'm10_prob_clo_home', 'cv10_prob_clo_home', 'm10_prob_clo_away', 'cv10_prob_clo_away', 'm30_prob_clo_home', 'cv30_prob_clo_home', 'm30_prob_clo_away', 'cv30_prob_clo_away', 'avg_home_score', 'avg_away_score', 'poisson_home_zero', 'poisson_away_zero', 'poisson_0x0', 'poisson_home_score', 'poisson_away_score', 'poisson_H/A', 'poissonha/ha', 'xPts_home', 'xPts_away', 'm10_xPts_home', 'm10_xPts_away', 'm10_Pts/xPts_dif_home', 'm10_Pts/xPts_dif_away', 'm10_Pts/xPts_rt_home', 'm10_Pts/xPts_rt_away', 'm5_xPts_home', 'm5_xPts_away', 'm5_Pts/xPts_dif_home', 'm5_Pts/xPts_dif_away', 'm5_Pts/xPts_rt_home', 'm5_Pts/xPts_rt_away', 'evo_Pts/xPts_rt_home', 'evo_Pts/xPts_rt_away', 'evo_Pts/xPts_rtdif_home', 'evo_Pts/xPts_rtdif_away', 'evo_Pts/xPts_dif_home', 'evo_Pts/xPts_dif_away', 'CVMO', 'CVMO_clo', 'm10_PLH_home', 'm10_PLA_away', 'm20_PLH_home', 'm20_PLA_away', 'm10_PLD_home', 'm10_PLD_away', 'm20_PLD_home', 'm20_PLD_away', 'm10_PLH_home_op', 'm10_PLA_away_op', 'm20_PLH_home_op', 'm20_PLA_away_op', 'm10_PLD_home_op', 'm10_PLD_away_op', 'm20_PLD_home_op', 'm20_PLD_away_op', 'pH/pA', 'pH/pA_clo', 'pH/pD', 'AC_PLAhh_op', 'AC_PLAha_op', 'Data', 'm5_PLAhOver_home', 'm5_PLAhOver_away', 'm10_PLAhOver_home', 'm10_PLAhOver_away', 'm5_PLAhh_home', 'm5_PLAha_away', 'm10_PLAhh_home', 'm10_PLAha_away', 'm20_PLAhh_home', 'm20_PLAha_away', 'm5_PLAhOver_home_op', 'm5_PLAhOver_away_op', 'm10_PLAhOver_home_op', 'm10_PLAhOver_away_op', 'm5_PLAhh_home_op', 'm5_PLAha_away_op', 'm10_PLAhh_home_op', 'm10_PLAha_away_op', 'm20_PLAhh_home_op', 'm20_PLAha_away_op', 'evo_PLAhh_home_10-20', 'evo_PLAha_away_10-20', 'opclos10_PLAhh_home', 'opclos10_PLAha_away', 'opclos10_PLAhOver_home', 'opclos10_PLAhOver_away', 'var_opclos_h', 'var_opclos_d', 'var_opclos_a', 'feature 8', 'feature 6', 'feature 14', 'feature 13', 'm3_varopclos_d_home', 'm3_varopclos_d_away', 'm1_varopclos_d_home', 'm1_varopclos_d_away', 'm3_varopclos_a_home', 'm3_varopclos_a_away', 'm1_varopclos_a_home', 'm1_varopclos_a_away', 'feature 7', 'feature 2', 'feature 10', 'feature 3', 'm5_varopclos_d_home', 'm5_varopclos_d_away', 'm10_varopclos_d_home', 'm10_varopclos_d_away', 'feature 5', 'feature 1', 'feature 9', 'feature 4', 'caiu_opclos_h', 'caiu_opclos_d', 'caiu_opclos_a', 'caiu_opclos_o', 'm100_caiuH_div', 'm100_caiuD_div', 'm100_caiuA_div', 'm100_caiuO_div', 'feature 12', 'm300_caiuD_div', 'm300_caiuA_div', 'm300_caiuO_div', 'Div - Ah_op', 'feature 11', 'm100_caiuD_divahop', 'm100_caiuA_divahop', 'm100_caiuO_divahop', 'Angle_HomeAway', 'Angle_HomeDraw', 'Angle_DrawAway', 'Angle_HomeAway_clo', 'Angle_HomeDraw_clo', 'Angle_DrawAway_clo']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrando colunas presentes\n",
    "str(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting main columns\n",
    "df2 = df[['id', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'oddH_op', 'oddD_op', 'oddA_op', 'oddH', 'oddD', 'oddA', 'Ah_op', 'oddAHH_op', 'oddAHA_op', 'Ah', 'oddAHH', 'oddAHA', 'AhOU_op', 'oddAHOver_op', 'oddAHUnder_op', 'AhOU', 'oddAHOver', 'oddAHUnder', 'Div Rank', 'FTR', 'count_div', 'pHome', 'pDraw', 'pAway', 'pHome_clo', 'pDraw_clo', 'pAway_clo','OverScore','PLH', 'PLD', 'PLA', 'PLH_op', 'PLD_op', 'PLA_op', 'dif', 'PL_Ahh', 'PL_Aha', 'PL_Ahh_op', 'PL_Aha_op','PL_AhOver', 'PL_AhUnder', 'PL_AhOver_op', 'PL_AhUnder_op','exuniclo10_dif', 'ex_uniclop10_dif', 'm10_Pts/xPts_rt_home', 'm10_Pts/xPts_rt_away', 'm5_Pts/xPts_rt_home', 'm5_Pts/xPts_rt_away', 'caiu_opclos_h',\n",
    "          'feature 1', 'feature 2', 'feature 3', 'feature 4', \n",
    "          'feature 5', 'feature 6', 'feature 7', 'feature 8', \n",
    "          'feature 9', 'feature 10', 'feature 11', 'feature 12', \n",
    "          'feature 13', 'feature 14']]\n",
    "\n",
    "# Selecting features that are relevant for this scenario\n",
    "features = [\n",
    "    'feature 1', 'feature 2', 'feature 3', 'feature 4', \n",
    "    'feature 5', 'feature 6', 'feature 7', 'feature 8', \n",
    "    'feature 9', 'feature 10', 'feature 11', 'feature 12', \n",
    "    'feature 13', 'feature 14'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to apply binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will not be used here, since the features selected are already normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar o binning\n",
    "def apply_binning(df, column_names, bins):\n",
    "    \"\"\"\n",
    "    Aplica binning às colunas especificadas do DataFrame.\n",
    "    \n",
    "    :param df: DataFrame original.\n",
    "    :param column_names: Lista de colunas para aplicar o binning.\n",
    "    :param bins: Número de bins ou uma lista especificando os limites dos bins.\n",
    "    \"\"\"\n",
    "    for column in column_names:\n",
    "        df[f'{column}'] = pd.cut(df[column], bins=bins, labels=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Spliting the dataframe into two major pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One dataset will be used to train the model and the other will be used to backtest our model & strategy in an out-of-sample scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in the training DataFrame: 86412\n",
      "Rows in the final test DataFrame: 67568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df_train, df_final_test = split_df_bydate(df2, 'Date', '2023-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping rows with NaN\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are evalueating the importance of each of the fourteen features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Target : 'caiu_opclos_h' (This is a binary type target. 1 means that the odds for the home team have dropped, while 0 means that they remained the same or have increased)\n",
    "\n",
    "- As explained in the README, we are looking for bets that have high probability of dropping between the market opening and closing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'caiu_opclos_h'\n",
    "\n",
    "# Separate the target before filtering features\n",
    "y = df_train[target]\n",
    "df_train_filtered = df_train[features]\n",
    "\n",
    "# Ensure the target column is not in the filtered DataFrame\n",
    "X = df_train_filtered\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Feature Importance - Random Forest Classifier Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier method was chosen because we wanted to capture interactions between features (some of which are correlated) and because our data has complex relationships that are not easily captured by univariate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feature 6</td>\n",
       "      <td>0.075314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feature 2</td>\n",
       "      <td>0.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature 8</td>\n",
       "      <td>0.074315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature 1</td>\n",
       "      <td>0.073305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feature 3</td>\n",
       "      <td>0.073021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feature 10</td>\n",
       "      <td>0.072317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feature 5</td>\n",
       "      <td>0.072028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>feature 7</td>\n",
       "      <td>0.071674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feature 9</td>\n",
       "      <td>0.071372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>feature 4</td>\n",
       "      <td>0.070913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>feature 14</td>\n",
       "      <td>0.068784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>feature 11</td>\n",
       "      <td>0.068386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>feature 13</td>\n",
       "      <td>0.068070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>feature 12</td>\n",
       "      <td>0.065301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature  Importance\n",
       "0    feature 6    0.075314\n",
       "1    feature 2    0.075200\n",
       "2    feature 8    0.074315\n",
       "3    feature 1    0.073305\n",
       "4    feature 3    0.073021\n",
       "5   feature 10    0.072317\n",
       "6    feature 5    0.072028\n",
       "7    feature 7    0.071674\n",
       "8    feature 9    0.071372\n",
       "9    feature 4    0.070913\n",
       "10  feature 14    0.068784\n",
       "11  feature 11    0.068386\n",
       "12  feature 13    0.068070\n",
       "13  feature 12    0.065301"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Getting feature importances\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature 6',\n",
       " 'feature 2',\n",
       " 'feature 8',\n",
       " 'feature 1',\n",
       " 'feature 3',\n",
       " 'feature 10',\n",
       " 'feature 5',\n",
       " 'feature 7',\n",
       " 'feature 9',\n",
       " 'feature 4',\n",
       " 'feature 14',\n",
       " 'feature 11',\n",
       " 'feature 13',\n",
       " 'feature 12']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df['Feature'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the feature importance metrics we have chosen the 10 best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'feature 6',\n",
    "    'feature 2',\n",
    "    'feature 8',\n",
    "    'feature 1',\n",
    "    'feature 3',\n",
    "    'feature 10',\n",
    "    'feature 5',\n",
    "    'feature 7',\n",
    "    'feature 9',\n",
    "    'feature 4'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X_train, X_test, y_train, y_test, models):\n",
    "    # Inicializando uma lista para armazenar os resultados\n",
    "    results = []\n",
    "\n",
    "    # Iterando sobre o dicionário de modelos\n",
    "    for name, model in models.items():\n",
    "        # Treinando o modelo com o conjunto de treinamento\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Realizando previsões no conjunto de teste\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculando as métricas\n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "        recall = recall_score(y_test, y_pred, average='binary')\n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Armazenando os resultados em uma lista\n",
    "        results.append({\n",
    "            'Model': name, \n",
    "            'Precision': precision, \n",
    "            'Recall': recall, \n",
    "            'F1 Score': f1, \n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "\n",
    "    # Convertendo a lista de resultados em um DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Retornando o DataFrame de resultados\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista (dicionário) dos modelos a serem avaliados\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVC': SVC(),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Supondo que df_train_filtered já foi criado e contém apenas as colunas de interesse\n",
    "X = df_train_filtered.drop('caiu_opclos_h', axis=1)[features]\n",
    "y = df_train_filtered['caiu_opclos_h']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Avaliando os modelos\n",
    "results_df = evaluate_models(X_train, X_test, y_train, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.285057</td>\n",
       "      <td>0.378049</td>\n",
       "      <td>0.605924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.603073</td>\n",
       "      <td>0.180460</td>\n",
       "      <td>0.277794</td>\n",
       "      <td>0.605763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.482399</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.452046</td>\n",
       "      <td>0.566806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.431784</td>\n",
       "      <td>0.441379</td>\n",
       "      <td>0.436529</td>\n",
       "      <td>0.521249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.520845</td>\n",
       "      <td>0.349425</td>\n",
       "      <td>0.418253</td>\n",
       "      <td>0.591597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision    Recall  F1 Score  Accuracy\n",
       "0  Logistic Regression   0.561086  0.285057  0.378049  0.605924\n",
       "1                  SVC   0.603073  0.180460  0.277794  0.605763\n",
       "2                  KNN   0.482399  0.425287  0.452046  0.566806\n",
       "3        Decision Tree   0.431784  0.441379  0.436529  0.521249\n",
       "4        Random Forest   0.520845  0.349425  0.418253  0.591597"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Testing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m5_varopclos_h_away',\n",
       " 'm10_varopclos_h_away',\n",
       " 'm5_varopclos_a_away',\n",
       " 'm5_varopclos_h_home',\n",
       " 'm10_varopclos_a_away',\n",
       " 'm10_varopclos_h_home',\n",
       " 'm5_varopclos_a_home',\n",
       " 'm10_varopclos_a_home',\n",
       " 'm100_caiuH_divahop',\n",
       " 'm300_caiuH_div']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Precisão: 0.52479815455594, Recall: 0.3486590038314176\n",
      "LogisticRegression - Precisão: 0.5571112748710391, Recall: 0.2896551724137931\n"
     ]
    }
   ],
   "source": [
    "# Preparando os dados\n",
    "X = df_train_filtered.drop('caiu_opclos_h', axis=1)[features]\n",
    "y = df_train_filtered['caiu_opclos_h']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Padronizando os dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Treinando e salvando o modelo RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "joblib.dump(rf_model, 'RF_CAIUH_v3.5.joblib')\n",
    "joblib.dump(scaler, 'scaler_RF_CAIUH_v3.5.joblib')  # Salvando o scaler usado pelo RF\n",
    "\n",
    "# Treinando e salvando o modelo LogisticRegression\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "joblib.dump(lr_model, 'LR_CAIUH_v3.5.joblib')  # Ajustando o nome para refletir o modelo de LogisticRegression\n",
    "\n",
    "# Opcional: Avaliando os modelos\n",
    "# RandomForest\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "print(f\"RandomForest - Precisão: {precision_rf}, Recall: {recall_rf}\")\n",
    "\n",
    "# LogisticRegression\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "precision_lr = precision_score(y_test, y_pred_lr)\n",
    "recall_lr = recall_score(y_test, y_pred_lr)\n",
    "print(f\"LogisticRegression - Precisão: {precision_lr}, Recall: {recall_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test.dropna(subset=features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       predicted_caiu_opclos_h_rf  probability_caiu_opclos_h_rf  \\\n",
      "86412                           0                          0.24   \n",
      "86413                           0                          0.46   \n",
      "86414                           0                          0.28   \n",
      "86415                           0                          0.23   \n",
      "86416                           1                          0.55   \n",
      "\n",
      "       predicted_caiu_opclos_h_lr  probability_caiu_opclos_h_lr  \n",
      "86412                           0                      0.194195  \n",
      "86413                           1                      0.611508  \n",
      "86414                           0                      0.312451  \n",
      "86415                           0                      0.345682  \n",
      "86416                           0                      0.463105  \n"
     ]
    }
   ],
   "source": [
    "features = ['m5_varopclos_h_away',\n",
    " 'm10_varopclos_h_away',\n",
    " 'm5_varopclos_a_away',\n",
    " 'm5_varopclos_h_home',\n",
    " 'm10_varopclos_a_away',\n",
    " 'm10_varopclos_h_home',\n",
    " 'm5_varopclos_a_home',\n",
    " 'm10_varopclos_a_home',\n",
    " 'm100_caiuH_divahop',\n",
    " 'm300_caiuH_div']\n",
    "\n",
    "# Carregando os modelos e o scaler salvos\n",
    "rf_model_loaded = joblib.load('RF_CAIUH_v3.5.joblib')\n",
    "lr_model_loaded = joblib.load('LR_CAIUH_v3.5.joblib')\n",
    "scaler_loaded = joblib.load('scaler_RF_CAIUH_v3.5.joblib')  # Supondo que o mesmo scaler é usado para ambos\n",
    "\n",
    "# Preparando df_final_test (certifique-se de que está filtrado corretamente)\n",
    "X_final_test = df_final_test[features]  # Ajuste conforme necessário\n",
    "X_final_test_scaled = scaler_loaded.transform(X_final_test)\n",
    "\n",
    "# Fazendo previsões com RandomForest\n",
    "y_pred_rf = rf_model_loaded.predict(X_final_test_scaled)\n",
    "y_pred_proba_rf = rf_model_loaded.predict_proba(X_final_test_scaled)[:, 1]  # Probabilidades da classe positiva\n",
    "\n",
    "# Fazendo previsões com LogisticRegression\n",
    "y_pred_lr = lr_model_loaded.predict(X_final_test_scaled)\n",
    "# Para LogisticRegression, as probabilidades também podem ser obtidas\n",
    "y_pred_proba_lr = lr_model_loaded.predict_proba(X_final_test_scaled)[:, 1]\n",
    "\n",
    "# Adicionando as previsões e os scores ao df_final_test para cada modelo\n",
    "df_final_test['predicted_caiu_opclos_h_rf'] = y_pred_rf\n",
    "df_final_test['probability_caiu_opclos_h_rf'] = y_pred_proba_rf\n",
    "df_final_test['predicted_caiu_opclos_h_lr'] = y_pred_lr\n",
    "df_final_test['probability_caiu_opclos_h_lr'] = y_pred_proba_lr\n",
    "\n",
    "# Exibindo as primeiras linhas para verificação\n",
    "print(df_final_test[['predicted_caiu_opclos_h_rf', 'probability_caiu_opclos_h_rf', 'predicted_caiu_opclos_h_lr', 'probability_caiu_opclos_h_lr']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test.to_excel(\"./Estudos/RF and LR - CAIU H v3.5.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
